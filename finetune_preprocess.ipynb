{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684873c5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from parser import DFG_java,DFG_csharp,DFG_python\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index, \n",
    "                   detokenize_code, tree_to_token_nodes)\n",
    "import sys\n",
    "if 'tree_sitter' in sys.modules:\n",
    "    del sys.modules['tree_sitter']\n",
    "    \n",
    "from tree_sitter import Language, Parser\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def read_examples(split='train'):\n",
    "    if (split=='valid') or (split=='dev'):\n",
    "        split='validation'\n",
    "    \n",
    "    # Simplest possible load - no extra parameters\n",
    "    dataset = load_dataset('code_x_glue_cc_code_to_code_trans', split=split)\n",
    "    \n",
    "    examples = []\n",
    "    for example in dataset:\n",
    "        examples.append({\n",
    "            'id': example['id'],\n",
    "            'java': example['java'],\n",
    "            'cs': example['cs']\n",
    "        })\n",
    "    \n",
    "    print(f'Loaded {len(examples)} examples from {split} split')\n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "def get_tokenizer_chars(tokenizer):\n",
    "    tokenizer_chars = []\n",
    "    for i in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.decode(i)\n",
    "        if len(token)==1:\n",
    "            tokenizer_chars.append(token)\n",
    "    tokenizer_chars = [c for c in tokenizer_chars if c!='�']\n",
    "    return tokenizer_chars\n",
    "\n",
    "\n",
    "def tokenize_codes_texts(texts, batch_size=1024):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    tokenizer_chars = get_tokenizer_chars(tokenizer)\n",
    "    texts = [''.join(filter(lambda c:c in tokenizer_chars, text)) for text in texts]\n",
    "    N = len(texts)\n",
    "    tokenized_texts = []\n",
    "    for start in range(0, len(texts),batch_size):\n",
    "        tokenized_texts += tokenizer(texts[start:start+batch_size]).input_ids\n",
    "    return tokenized_texts\n",
    "\n",
    "\n",
    "def length_stats(s, title=\"Stats\"):\n",
    "    # Check if s contains lists/strings or numbers\n",
    "    first_val = s.iloc[0] if len(s) > 0 else None\n",
    "    \n",
    "    if isinstance(first_val, (list, str)):\n",
    "        # For lists/strings, get lengths\n",
    "        lens = s.apply(len)\n",
    "    else:\n",
    "        # For numeric data, use values directly\n",
    "        lens = s\n",
    "    \n",
    "    print(title)\n",
    "    print('mean=%.1f, median=%.1f, std=%.1f, max=%.1f, min=%.1f' %\n",
    "          (lens.mean(), lens.median(), lens.std(), lens.max(), lens.min()))\n",
    "    \n",
    "    # Plotting disabled\n",
    "    \n",
    "    \n",
    "def get_code_tokens_ranges(data, code_col, code_tokens_col):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    pbar = data.itertuples()\n",
    "    ranges = []\n",
    "    \n",
    "    for row in pbar:\n",
    "        code_tokens = [tokenizer.decode(ct) for ct in getattr(row, code_tokens_col)][1:-1] # 1:-1 to remove <s> and </s>\n",
    "        code2 = ''.join(code_tokens) # may miss some spaces / special chars that are in row.code_col\n",
    "        code = getattr(row, code_col)\n",
    "        \n",
    "        # map each position in code2 to a position in code\n",
    "        code2_to_code = []\n",
    "        j=0\n",
    "        for i in range(len(code2)):\n",
    "            while code2[i]!=code[j]:\n",
    "                j += 1\n",
    "            code2_to_code.append(j)\n",
    "            \n",
    "        # map each code token to a range in code\n",
    "        code2_idx = 0\n",
    "        curr_ranges = []\n",
    "        for ct in code_tokens:\n",
    "            s,e = code2_idx, code2_idx+len(ct)\n",
    "            code2_idx = e\n",
    "            curr_ranges.append((min(code2_to_code[s:e]),1+max(code2_to_code[s:e])))\n",
    "        ranges.append([None]+curr_ranges+[None]) # first and last for <s> and </s>\n",
    "        \n",
    "    data[code_tokens_col+'_ranges'] = ranges\n",
    "    \n",
    "    \n",
    "def extract_structure(code, parser):\n",
    "    # ast\n",
    "    tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "    root_node = tree.root_node  \n",
    "    ast_token_nodes = tree_to_token_nodes(root_node) # leaves\n",
    "    \n",
    "    # dfg\n",
    "    tokens_index = [(node.start_point, node.end_point) for node in ast_token_nodes]\n",
    "    code=code.split('\\n')\n",
    "    code_tokens=[index_to_code_token(x,code) for x in tokens_index] \n",
    "    index_to_code={index:(idx,code_) for idx,(index,code_) in enumerate(zip(tokens_index,code_tokens))}\n",
    "    try:\n",
    "        DFG,_ = parser[1](root_node,index_to_code,{}) \n",
    "    except:\n",
    "        DFG = []\n",
    "    for d in DFG:\n",
    "        assert (d[2]=='comesFrom' or d[2]=='computedFrom')\n",
    "    DFG = [(d[1], d[4]) for d in DFG if (len(d[4])>0)] # left comes from right\n",
    "    return code_tokens, ast_token_nodes, DFG\n",
    "\n",
    "\n",
    "def format_node_ranges(code, nodes):\n",
    "    line_lens = [len(line)+1 for line in code.split('\\n')]\n",
    "    line_starts = [0] + list(np.cumsum(line_lens))\n",
    "    return [(line_starts[node.start_point[0]]+node.start_point[1],\n",
    "             line_starts[node.end_point[0]]+node.end_point[1]) for node in nodes]\n",
    "    \n",
    "    \n",
    "def add_structure(data, lang):\n",
    "    LANGUAGE = Language('parser/my-languages2.so', 'c_sharp' if lang=='cs' else lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    dfg_function={'python':DFG_python, 'java':DFG_java, 'cs':DFG_csharp, 'c_sharp':DFG_csharp}\n",
    "    parser = [parser, dfg_function[lang]]\n",
    "        \n",
    "    ast_leaf_tokens, ast_leaves, ast_leaf_ranges, dfg_edges = [], [], [], []\n",
    "    for code in data[lang]:\n",
    "        curr_code_tokens, curr_ast_leaves, curr_dfg_edges = extract_structure(code, parser)\n",
    "        ast_leaf_tokens.append(curr_code_tokens)\n",
    "        ast_leaves.append(curr_ast_leaves)\n",
    "        ast_leaf_ranges.append(format_node_ranges(code, curr_ast_leaves))\n",
    "        dfg_edges.append(curr_dfg_edges)\n",
    "        \n",
    "    data[lang+'_ast_leaves'] = ast_leaves # list of leaf nodes\n",
    "    data[lang+'_dfg_edges'] = dfg_edges # list of \"left leaf node index comes from right leaf nodes indices\"\n",
    "    data[lang+'_ast_leaf_tokens'] = ast_leaf_tokens # list of code substrings corresponding to each leaf\n",
    "    data[lang+'_ast_leaf_ranges'] = ast_leaf_ranges # list of (start,end) in code for each leaf node\n",
    "    \n",
    "    print ('# '+lang+' samples with failed/empty DFG:', (data[lang+'_dfg_edges'].apply(len)==0).sum())\n",
    "    \n",
    "    \n",
    "def overlap(s1,e1,s2,e2):\n",
    "    return s1<=s2<e1 or s2<=s1<e2\n",
    "    \n",
    "    \n",
    "def get_leaf_code_token_indices(data, lang):\n",
    "    ast_leaf_token_idxs = []\n",
    "    pbar = data.itertuples() if len(data)<=100000 else tqdm(data.itertuples())\n",
    "    for row in pbar:\n",
    "        ast_leaf_token_idxs.append([])\n",
    "        code_tokens_last_idx = len(getattr(row, lang+'_tokens'))-1\n",
    "        code_tokens_ranges = getattr(row, lang+'_tokens_ranges')\n",
    "        for s,e in getattr(row, lang+'_ast_leaf_ranges'):\n",
    "            if s==e: # there are leaves with start_point=end_point\n",
    "                ast_leaf_token_idxs[-1].append([])\n",
    "                continue\n",
    "            j = 1\n",
    "            while not(overlap(s,e,code_tokens_ranges[j][0],code_tokens_ranges[j][1])):\n",
    "                j += 1\n",
    "                if j==code_tokens_last_idx: # can't find code tokens for this leaf\n",
    "                    break\n",
    "            if j==code_tokens_last_idx: # can't find code tokens for this leaf\n",
    "                ast_leaf_token_idxs[-1].append([])\n",
    "                continue\n",
    "            curr_leaf_token_idxs = []\n",
    "            while overlap(s,e,code_tokens_ranges[j][0],code_tokens_ranges[j][1]):\n",
    "                curr_leaf_token_idxs.append(j)\n",
    "                j += 1\n",
    "                if j==code_tokens_last_idx:\n",
    "                    break\n",
    "            ast_leaf_token_idxs[-1].append(curr_leaf_token_idxs)\n",
    "    data[lang+'_ast_leaf_code_token_idxs'] = ast_leaf_token_idxs\n",
    "    print ('Average # leaves with no matching code tokens:', \n",
    "           data[lang+'_ast_leaf_code_token_idxs'].apply(lambda x:sum([1 for xi in x if xi==[]])).mean())\n",
    "    \n",
    "\n",
    "def get_lr_path(leaf):\n",
    "    path = [leaf]\n",
    "    while path[-1].parent is not None:\n",
    "        path.append(path[-1].parent)\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_ll_sim(p1, p2): \n",
    "    common = 1\n",
    "    for i in range(2, min(len(p1), len(p2))+1):\n",
    "        if p1[-i]==p2[-i]:\n",
    "            common += 1\n",
    "        else:\n",
    "            break\n",
    "    return common*common / (len(p1)*len(p2))   \n",
    "\n",
    "\n",
    "def get_ast_lr_paths_and_ll_sim(data, lang):\n",
    "    sims = []\n",
    "    lr_paths = []\n",
    "    for i,row in tqdm(enumerate(data.itertuples())):\n",
    "        ast_leaves = getattr(row, lang+'_ast_leaves')\n",
    "        L = min(len(ast_leaves), 512)\n",
    "        curr_paths = [get_lr_path(leaf) for leaf in ast_leaves]\n",
    "        curr_sims = np.ones((L,L))\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                curr_sims[i,j] = curr_sims[j,i] = get_ll_sim(curr_paths[i], curr_paths[j])\n",
    "        sims.append(curr_sims)\n",
    "        lr_paths.append([[node.type for node in path] for path in curr_paths])\n",
    "    data.drop(columns=[lang+'_ast_leaves'], inplace=True)\n",
    "    data[lang+'_ll_sims'] = sims\n",
    "    data[lang+'_lr_paths_types'] = lr_paths\n",
    "\n",
    "\n",
    "def process_dfg_edges(data, lang):\n",
    "    dfg_node_code_token_idxs = []\n",
    "    dfg_edges = []\n",
    "    pbar = data.itertuples()  \n",
    "    for row in pbar:\n",
    "        curr_dfg_edges = getattr(row, lang+'_dfg_edges')\n",
    "        if len(curr_dfg_edges)>0:\n",
    "            nodes = sorted(list(set(np.concatenate([[left]+right for left,right in curr_dfg_edges]))))\n",
    "        else:\n",
    "            nodes = []\n",
    "        node_to_idx = {k:i for i,k in enumerate(nodes)}\n",
    "        ast_leaf_code_token_idxs = getattr(row, lang+'_ast_leaf_code_token_idxs')\n",
    "        dfg_node_code_token_idxs.append( [ast_leaf_code_token_idxs[i] for i in nodes] )\n",
    "        dfg_edges.append( [(node_to_idx[left], [node_to_idx[r] for r in right]) for left,right in curr_dfg_edges] )\n",
    "    data[lang+'_dfg_edges'] = dfg_edges\n",
    "    data[lang+'_dfg_node_code_token_idxs'] = dfg_node_code_token_idxs\n",
    "    \n",
    "    \n",
    "def some_more_stats(data, lang):\n",
    "    node_types = set(np.concatenate(list(data[lang+'_lr_paths_types'].apply(lambda ll:np.concatenate(ll)))))\n",
    "    print ('# node types:', len(node_types))\n",
    "    if 'ERROR' in node_types:\n",
    "        num_error_nodes = data[lang+'_lr_paths_types'].apply(lambda paths:\n",
    "                        np.mean(['ERROR' in path for path in paths]))\n",
    "        print ('Distrubution of fraction of leaf-root paths with ERROR node in one code')\n",
    "        length_stats(num_error_nodes)\n",
    "    print ('Distrubution of AST depth')\n",
    "    length_stats(data[lang+'_lr_paths_types'].apply(lambda paths:max([len(p) for p in paths])))          \n",
    "    print ('Distrubution of # ast leaves per code')\n",
    "    length_stats(data[lang+'_ast_leaf_code_token_idxs'].apply(len))\n",
    "    print ('Distrubution of # dfg nodes per code')\n",
    "    length_stats(data[lang+'_dfg_node_code_token_idxs'].apply(len))\n",
    "    print ('Distrubution of # dfg edges per code')\n",
    "    def num_dfg_edges(s):\n",
    "        if s==[]:\n",
    "            return 0\n",
    "        return sum([len(rights) for _,rights in s])\n",
    "    length_stats(data[lang+'_dfg_edges'].apply(num_dfg_edges))\n",
    "    return node_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29513c",
   "metadata": {},
   "source": [
    "## CodeXGLUE translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4461033d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Split=train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_x_glue_cc_code_to_code_trans (/Users/cris/.cache/huggingface/datasets/code_x_glue_cc_code_to_code_trans/default/0.0.0/86dd57d2b1e88c6e589646133b76f2fef9d56c82e933d7f276e8a5b60ab18c34)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10300 examples from train split\n",
      "# java samples with failed/empty DFG: 1457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #java_tokens\n",
      "mean=48.4, median=33.0, std=44.2, max=394.0, min=11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.459126213592233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10300it [00:06, 1490.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 177\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=8.6, median=8.0, std=2.2, max=39.0, min=5.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=38.8, median=22.0, std=38.9, max=314.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=8.1, median=5.0, std=10.9, max=117.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=7.2, median=4.0, std=12.0, max=172.0, min=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cs samples with failed/empty DFG: 1192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #cs_tokens\n",
      "mean=59.4, median=55.0, std=44.8, max=407.0, min=12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.47126213592233007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10300it [00:09, 1094.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 210\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=10.8, median=11.0, std=2.7, max=28.0, min=4.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=46.2, median=45.0, std=38.7, max=320.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=11.7, median=10.0, std=12.0, max=119.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=10.4, median=7.0, std=17.0, max=1024.0, min=0.0\n",
      "\n",
      "\n",
      "Split=validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_x_glue_cc_code_to_code_trans (/Users/cris/.cache/huggingface/datasets/code_x_glue_cc_code_to_code_trans/default/0.0.0/86dd57d2b1e88c6e589646133b76f2fef9d56c82e933d7f276e8a5b60ab18c34)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 examples from validation split\n",
      "# java samples with failed/empty DFG: 70\n",
      "Distribution of #java_tokens\n",
      "mean=51.7, median=34.0, std=49.4, max=318.0, min=12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:00, 1290.55it/s]\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 145\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=8.7, median=8.0, std=2.3, max=21.0, min=5.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=42.3, median=22.0, std=43.1, max=261.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=9.2, median=5.0, std=12.5, max=79.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=8.6, median=4.0, std=15.5, max=176.0, min=0.0\n",
      "# cs samples with failed/empty DFG: 51\n",
      "Distribution of #cs_tokens\n",
      "mean=63.8, median=62.0, std=49.4, max=324.0, min=13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:00, 927.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 150\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=11.1, median=11.0, std=2.8, max=24.0, min=6.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=50.7, median=45.0, std=43.0, max=282.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=13.2, median=14.0, std=13.8, max=91.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=12.0, median=12.0, std=15.8, max=119.0, min=0.0\n",
      "\n",
      "\n",
      "Split=test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_x_glue_cc_code_to_code_trans (/Users/cris/.cache/huggingface/datasets/code_x_glue_cc_code_to_code_trans/default/0.0.0/86dd57d2b1e88c6e589646133b76f2fef9d56c82e933d7f276e8a5b60ab18c34)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 examples from test split\n",
      "# java samples with failed/empty DFG: 160\n",
      "Distribution of #java_tokens\n",
      "mean=47.6, median=33.0, std=41.6, max=347.0, min=12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 1551.46it/s]\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 153\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=8.6, median=8.0, std=2.1, max=19.0, min=5.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=38.5, median=22.0, std=38.0, max=306.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=8.0, median=5.0, std=10.5, max=73.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=7.0, median=4.0, std=11.2, max=90.0, min=0.0\n",
      "# cs samples with failed/empty DFG: 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #cs_tokens\n",
      "mean=58.4, median=55.0, std=41.5, max=332.0, min=12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 1183.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 174\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.1, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=10.7, median=11.0, std=2.7, max=22.0, min=6.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=45.8, median=45.0, std=37.4, max=270.0, min=10.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=11.5, median=9.5, std=11.3, max=82.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=10.1, median=7.0, std=11.9, max=90.0, min=0.0\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "#datasets.disable_caching()\n",
    "\n",
    "save_dir = 'data/codexglue_translation/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "all_node_types = set()\n",
    "data_by_split = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print ('\\n\\nSplit='+split)\n",
    "    data = read_examples(split) # id, java, cs\n",
    "    for lang in ['java', 'cs']:\n",
    "        add_structure(data, lang) # lang_ -> ast_leaves, dfg_edges, ast_leaf_tokens, ast_leaf_ranges\n",
    "        data[lang+'_tokens'] = tokenize_codes_texts(list(data[lang]))\n",
    "        length_stats(data[lang+'_tokens'], 'Distribution of #'+lang+'_tokens')\n",
    "        get_code_tokens_ranges(data, lang, lang+'_tokens') # list of (start,end) one for each code_token\n",
    "        data.drop(columns=[lang], inplace=True)\n",
    "        get_leaf_code_token_indices(data, lang)\n",
    "        data.drop(columns=[lang+c for c in ['_ast_leaf_tokens', '_ast_leaf_ranges', '_tokens_ranges']], \n",
    "                  inplace=True)\n",
    "        get_ast_lr_paths_and_ll_sim(data, lang)\n",
    "        process_dfg_edges(data, lang)\n",
    "        more_node_types = some_more_stats(data, lang)\n",
    "        all_node_types.update(more_node_types)\n",
    "    data_by_split[split] = data\n",
    "    \n",
    "# Map node types to indices.\n",
    "all_node_types = sorted(list(all_node_types))\n",
    "node_type_to_ind = {t:i for i,t in enumerate(all_node_types)}\n",
    "pickle.dump(all_node_types, open(save_dir+'all_node_types.pkl', 'wb'))\n",
    "\n",
    "# Convert node types on paths to indices.\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    for lang in ['java', 'cs']:\n",
    "        data_by_split[split][lang+'_lr_paths_types'] = data_by_split[split][lang+'_lr_paths_types'].apply(\n",
    "                                            lambda ll: [[node_type_to_ind[t] for t in path] for path in ll])\n",
    "        \n",
    "# Save data. Not converting array cols to strings, storing with pickle.\n",
    "pickle.dump(data_by_split, open(save_dir+'preprocessed_data_by_split.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0461e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/cris/Desktop/StructCoder/.venv/bin/python\n",
      "Python version: 3.10.17 (main, May 22 2025, 01:38:43) [Clang 20.1.4 ]\n",
      "tree-sitter location: /Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "import tree_sitter\n",
    "print(f\"tree-sitter location: {tree_sitter.__file__}\")\n",
    "\n",
    "# Try to get version\n",
    "try:\n",
    "    from tree_sitter import LANGUAGE_VERSION\n",
    "    print(f\"tree-sitter LANGUAGE_VERSION: {LANGUAGE_VERSION}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ce328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser files:\n",
      "  my-languages.so.x86_backup: 6.6 MB, modified: 1764786776.5274994\n",
      "  my-languages2.so.x86_backup: 6.9 MB, modified: 1764786776.5358791\n",
      "  my-languages2.so: 11.4 MB, modified: 1769189794.7306406\n",
      "\n",
      "✅ Successfully loaded parser!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "parser_dir = Path('parser')\n",
    "\n",
    "# List all .so files\n",
    "print(\"Parser files:\")\n",
    "for f in parser_dir.glob('*.so*'):\n",
    "    print(f\"  {f.name}: {f.stat().st_size / 1024 / 1024:.1f} MB, modified: {f.stat().st_mtime}\")\n",
    "    \n",
    "    # Check if it's a symlink\n",
    "    if f.is_symlink():\n",
    "        print(f\"    → symlink to: {os.readlink(f)}\")\n",
    "\n",
    "# Test loading\n",
    "from tree_sitter import Language\n",
    "try:\n",
    "    lang = Language('parser/my-languages2.so', 'java')\n",
    "    print(\"\\n✅ Successfully loaded parser!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading parser: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d24491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset code_x_glue_cc_code_to_code_trans (/Users/cris/.cache/huggingface/datasets/code_x_glue_cc_code_to_code_trans/default/0.0.0/86dd57d2b1e88c6e589646133b76f2fef9d56c82e933d7f276e8a5b60ab18c34)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27bc449d0ea4e7aaa4e3d4371eb1eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_splits = load_dataset('code_x_glue_cc_code_to_code_trans')\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset = dataset_splits[split]\n",
    "    lines = {'java':[], 'cs':[]}\n",
    "    for eg in dataset:\n",
    "        lines['java'].append(eg['java'])\n",
    "        lines['cs'].append(eg['cs'])\n",
    "    for lang in ['java','cs']:\n",
    "        with open('data/codexglue_translation/'+split+'_'+lang+'.txt', 'w') as f:\n",
    "            lines[lang][-1] = lines[lang][-1][:-1] # to remove last empty line\n",
    "            f.writelines(lines[lang])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe97614",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## CodeXGLUE generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695bf2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Split=train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--code_x_glue_tc_text_to_code-d024203af31b2592\n",
      "Reusing dataset parquet (/Users/cris/.cache/huggingface/datasets/parquet/google--code_x_glue_tc_text_to_code-d024203af31b2592/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 examples from train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #text_tokens\n",
      "mean=215.4, median=176.0, std=154.0, max=2248.0, min=20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# java samples with failed/empty DFG: 35480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #java_tokens\n",
      "mean=35.4, median=25.0, std=25.7, max=266.0, min=8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.00645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:27, 3629.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 176\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.0, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=7.9, median=7.0, std=2.3, max=31.0, min=4.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=26.6, median=18.0, std=19.8, max=162.0, min=6.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=4.2, median=3.0, std=5.5, max=55.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=3.4, median=2.0, std=5.4, max=95.0, min=0.0\n",
      "\n",
      "\n",
      "Split=validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--code_x_glue_tc_text_to_code-d024203af31b2592\n",
      "Reusing dataset parquet (/Users/cris/.cache/huggingface/datasets/parquet/google--code_x_glue_tc_text_to_code-d024203af31b2592/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 examples from validation split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #text_tokens\n",
      "mean=187.1, median=153.0, std=128.5, max=1128.0, min=24.0\n",
      "# java samples with failed/empty DFG: 611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #java_tokens\n",
      "mean=41.1, median=33.0, std=28.6, max=192.0, min=8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 2858.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 153\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.0, max=0.1, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=8.4, median=8.0, std=2.4, max=20.0, min=4.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=31.0, median=23.0, std=22.4, max=153.0, min=6.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=5.3, median=3.0, std=6.6, max=61.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=4.5, median=2.0, std=7.5, max=136.0, min=0.0\n",
      "\n",
      "\n",
      "Split=test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration google--code_x_glue_tc_text_to_code-d024203af31b2592\n",
      "Reusing dataset parquet (/Users/cris/.cache/huggingface/datasets/parquet/google--code_x_glue_tc_text_to_code-d024203af31b2592/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 examples from test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #text_tokens\n",
      "mean=203.6, median=169.0, std=142.4, max=896.0, min=23.0\n",
      "# java samples with failed/empty DFG: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #java_tokens\n",
      "mean=2.0, median=2.0, std=0.0, max=2.0, min=2.0\n",
      "Average # leaves with no matching code tokens: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 489702.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 1\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=1.0, median=1.0, std=0.0, max=1.0, min=1.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=1.0, median=1.0, std=0.0, max=1.0, min=1.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.0, max=0.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=0.0, median=0.0, std=0.0, max=0.0, min=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "def read_examples(split):\n",
    "    # Map split names to HuggingFace naming convention\n",
    "    if split in ['valid', 'validation']:\n",
    "        split='validation'  # HuggingFace uses 'validation', not 'dev'\n",
    "    \n",
    "    # Load from HuggingFace instead of local files\n",
    "    dataset = load_dataset('google/code_x_glue_tc_text_to_code', split=split)\n",
    "    \n",
    "    examples = []\n",
    "    for idx, example in enumerate(dataset):\n",
    "        nl = example['nl'].strip()\n",
    "        code = example['code'].strip()\n",
    "        assert (code == remove_comments_and_docstrings(code, 'java'))\n",
    "        assert ('\\n' not in code)\n",
    "        examples.append([idx, nl, code])\n",
    "    \n",
    "    print(f'Loaded {len(examples)} examples from {split} split')\n",
    "    return pd.DataFrame(examples, columns=['id', 'nl', 'java'])\n",
    "\n",
    "save_dir = 'data/codexglue_generation/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "all_node_types = set()\n",
    "data_by_split = {}\n",
    "lang = 'java'\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print ('\\n\\nSplit='+split)\n",
    "    data = read_examples(split) # id, nl, java\n",
    "    data['nl_tokens'] = tokenize_codes_texts(list(data['nl']))\n",
    "    length_stats(data['nl_tokens'], 'Distribution of #text_tokens')\n",
    "    add_structure(data, 'java') # lang_ -> ast_leaves, dfg_edges, ast_leaf_tokens, ast_leaf_ranges\n",
    "    data[lang+'_tokens'] = tokenize_codes_texts(list(data[lang]))\n",
    "    length_stats(data[lang+'_tokens'], 'Distribution of #'+lang+'_tokens')\n",
    "    get_code_tokens_ranges(data, lang, lang+'_tokens') # list of (start,end) one for each code_token\n",
    "    with open(save_dir+split+'_'+lang+'.txt', 'w') as f:\n",
    "        lines = [code+'\\n' for code in data[lang]]\n",
    "        lines[-1] = lines[-1][:-1] # to remove last empty line\n",
    "        f.writelines(lines) \n",
    "    data.drop(columns=['nl', lang], inplace=True)\n",
    "    get_leaf_code_token_indices(data, lang)\n",
    "    data.drop(columns=[lang+c for c in ['_ast_leaf_tokens', '_ast_leaf_ranges', '_tokens_ranges']], inplace=True)\n",
    "    get_ast_lr_paths_and_ll_sim(data, lang)\n",
    "    process_dfg_edges(data, lang)\n",
    "    more_node_types = some_more_stats(data, lang)\n",
    "    all_node_types.update(more_node_types)\n",
    "    data_by_split[split] = data\n",
    "    \n",
    "# Map node types to indices.\n",
    "all_node_types = sorted(list(all_node_types))\n",
    "node_type_to_ind = {t:i for i,t in enumerate(all_node_types)}\n",
    "pickle.dump(all_node_types, open(save_dir+'all_node_types.pkl', 'wb'))\n",
    "\n",
    "# Convert node types on paths to indices.\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    data_by_split[split][lang+'_lr_paths_types'] = data_by_split[split][lang+'_lr_paths_types'].apply(\n",
    "                                            lambda ll: [[node_type_to_ind[t] for t in path] for path in ll])\n",
    "        \n",
    "# Save data. Not converting array cols to strings, storing with pickle.\n",
    "pickle.dump(data_by_split, open(save_dir+'preprocessed_data_by_split.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e08abb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## APPS generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49a6e565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Split=train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a534cca1193544a7b3c2186878cf3a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: apps_code/all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset apps_code/all to /Users/cris/.cache/huggingface/datasets/codeparrot___apps_code/all/0.0.0/04ac807715d07d6e5cc580f59cdc8213cd7dc4529d0bb819cca72c9f8e8c1aa5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163f4fc6a7834af79be121f30e7971d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121a6ace6ee24b6ab7ea04c79f38cc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71286e353f1a4cf6ad4677a0803dfc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ef1adcb50e4e72a1d6ed1c4749e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88431948281e4d7ab92d0ee854c8ad34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071cfceedc404d6780e4cb779b359872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset apps_code downloaded and prepared to /Users/cris/.cache/huggingface/datasets/codeparrot___apps_code/all/0.0.0/04ac807715d07d6e5cc580f59cdc8213cd7dc4529d0bb819cca72c9f8e8c1aa5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd30a197feba436c84447c13b46613d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of pairs 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #text_tokens\n",
      "mean=412.0, median=352.0, std=292.7, max=7203.0, min=28.0\n",
      "No. of pairs 117232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# python samples with failed/empty DFG: 696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #python_tokens\n",
      "mean=200.7, median=128.0, std=1749.6, max=358184.0, min=3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "117232it [02:58, 655.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # leaves with no matching code tokens: 10.979246280878941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117232it [12:35, 155.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# node types: 187\n",
      "Distrubution of fraction of leaf-root paths with ERROR node in one code\n",
      "Stats\n",
      "mean=0.1, median=0.0, std=0.2, max=1.0, min=0.0\n",
      "Distrubution of AST depth\n",
      "Stats\n",
      "mean=13.8, median=14.0, std=4.0, max=400.0, min=3.0\n",
      "Distrubution of # ast leaves per code\n",
      "Stats\n",
      "mean=127.5, median=95.0, std=151.5, max=5000.0, min=1.0\n",
      "Distrubution of # dfg nodes per code\n",
      "Stats\n",
      "mean=45.3, median=33.0, std=59.5, max=3391.0, min=0.0\n",
      "Distrubution of # dfg edges per code\n",
      "Stats\n",
      "mean=58.6, median=37.0, std=581.3, max=196872.0, min=0.0\n",
      "\n",
      "\n",
      "Split=test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: apps_code/all\n",
      "Reusing dataset apps_code (/Users/cris/.cache/huggingface/datasets/codeparrot___apps_code/all/0.0.0/04ac807715d07d6e5cc580f59cdc8213cd7dc4529d0bb819cca72c9f8e8c1aa5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b54d33ae5645719b6e10d4a62b4fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of pairs 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cris/Desktop/StructCoder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of #text_tokens\n",
      "mean=527.1, median=504.0, std=223.1, max=2003.0, min=56.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def read_examples(split):\n",
    "    examples = []\n",
    "    dataset = load_dataset(\"codeparrot/apps\")[split]\n",
    "    for eg in dataset:\n",
    "        if len(eg['starter_code'])>0:\n",
    "            form = 'Use call-based format'\n",
    "        else:\n",
    "            form = 'Use cell-based format'\n",
    "        if split=='train':\n",
    "            sols = []\n",
    "            for sol in eg['solutions'][2:-2].split('\", \"'):\n",
    "                # sol = reindent_code(sol)\n",
    "                sol = sol.replace('\\\\n', '\\n')\n",
    "                sols.append(sol)\n",
    "            examples.append([eg['problem_id'], eg['question']+'\\n'+eg['starter_code']+'\\n'+form, sols])\n",
    "        elif split=='test':\n",
    "            examples.append([eg['problem_id'], eg['question']+'\\n'+eg['starter_code']+'\\n'+form, None])\n",
    "    print ('No. of pairs', len(examples))     \n",
    "    return pd.DataFrame(examples, columns=['id', 'nl', 'python'])\n",
    "\n",
    "def get_leaf_code_token_indices_fast(data, lang):\n",
    "    ast_leaf_token_idxs = []\n",
    "    pbar = data.itertuples() if len(data)<=100000 else tqdm(data.itertuples())\n",
    "    for row in pbar:\n",
    "        jj = 1\n",
    "        ast_leaf_token_idxs.append([])\n",
    "        code_tokens_last_idx = len(getattr(row, lang+'_tokens'))-1\n",
    "        code_tokens_ranges = getattr(row, lang+'_tokens_ranges')\n",
    "        for s,e in getattr(row, lang+'_ast_leaf_ranges'):\n",
    "            if s==e or jj==code_tokens_last_idx: # there are leaves with start_point=end_point\n",
    "                ast_leaf_token_idxs[-1].append([])\n",
    "                continue\n",
    "            j = jj\n",
    "            while not(overlap(s,e,code_tokens_ranges[j][0],code_tokens_ranges[j][1])):\n",
    "                j += 1\n",
    "                if j==code_tokens_last_idx: # can't find code tokens for this leaf\n",
    "                    break\n",
    "            if j==code_tokens_last_idx: # can't find code tokens for this leaf\n",
    "                ast_leaf_token_idxs[-1].append([])\n",
    "                continue\n",
    "            curr_leaf_token_idxs = []\n",
    "            while overlap(s,e,code_tokens_ranges[j][0],code_tokens_ranges[j][1]):\n",
    "                curr_leaf_token_idxs.append(j)\n",
    "                j += 1\n",
    "                if j==code_tokens_last_idx:\n",
    "                    break\n",
    "            jj = j\n",
    "            ast_leaf_token_idxs[-1].append(curr_leaf_token_idxs)\n",
    "    data[lang+'_ast_leaf_code_token_idxs'] = ast_leaf_token_idxs\n",
    "    print ('Average # leaves with no matching code tokens:', \n",
    "           data[lang+'_ast_leaf_code_token_idxs'].apply(lambda x:sum([1 for xi in x if xi==[]])).mean())\n",
    "\n",
    "\n",
    "\n",
    "save_dir = 'data/apps_generation/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "all_node_types = set()\n",
    "data_by_split = {}\n",
    "lang = 'python'\n",
    "for split in ['train', 'test']:\n",
    "    print ('\\n\\nSplit='+split)\n",
    "    data = read_examples(split) # id, nl, python\n",
    "    data['nl_tokens'] = tokenize_codes_texts(list(data['nl']))\n",
    "    length_stats(data['nl_tokens'], 'Distribution of #text_tokens')\n",
    "    if split!='test':\n",
    "        data = pd.DataFrame([[row.id, row.nl, sol, row.nl_tokens] for row in data.itertuples() for sol in row.python], \n",
    "                            columns=data.columns)\n",
    "        print ('No. of pairs', len(data))  \n",
    "        add_structure(data, lang) # lang_ -> ast_leaves, dfg_edges, ast_leaf_tokens, ast_leaf_ranges\n",
    "        \n",
    "        # Keep a max of 5K leaves to speed up following code.\n",
    "        for col in [lang+'_'+c for c in ['ast_leaves', 'ast_leaf_tokens', 'ast_leaf_ranges']]:\n",
    "            data[col] = data[col].apply(lambda x:x[:5000])\n",
    "        def f(edges):\n",
    "            filtered_edges = []\n",
    "            for left,rights in edges:\n",
    "                if left<5000:\n",
    "                    rights = [r for r in rights if r<5000]\n",
    "                    if len(rights)>0:\n",
    "                        filtered_edges.append((left, rights))\n",
    "            return filtered_edges\n",
    "        data['python_dfg_edges'] = data['python_dfg_edges'].apply(f)\n",
    "            \n",
    "        data[lang+'_tokens'] = tokenize_codes_texts(list(data[lang]))\n",
    "        length_stats(data[lang+'_tokens'], 'Distribution of #'+lang+'_tokens')\n",
    "        get_code_tokens_ranges(data, lang, lang+'_tokens') # list of (start,end) one for each code_token\n",
    "        data.drop(columns=['nl', lang], inplace=True)\n",
    "        get_leaf_code_token_indices_fast(data, lang)\n",
    "        data.drop(columns=[lang+c for c in ['_ast_leaf_tokens', '_ast_leaf_ranges', '_tokens_ranges']], inplace=True)\n",
    "        get_ast_lr_paths_and_ll_sim(data, lang)\n",
    "        process_dfg_edges(data, lang)\n",
    "        more_node_types = some_more_stats(data, lang)\n",
    "        all_node_types.update(more_node_types)\n",
    "    data_by_split[split] = data\n",
    "    \n",
    "# Map node types to indices.\n",
    "all_node_types = sorted(list(all_node_types))\n",
    "node_type_to_ind = {t:i for i,t in enumerate(all_node_types)}\n",
    "pickle.dump(all_node_types, open(save_dir+'all_node_types.pkl', 'wb'))\n",
    "\n",
    "# Convert node types on paths to indices.\n",
    "for split in ['train']:\n",
    "    data_by_split[split][lang+'_lr_paths_types'] = data_by_split[split][lang+'_lr_paths_types'].apply(\n",
    "                                            lambda ll: [[node_type_to_ind[t] for t in path] for path in ll])\n",
    "        \n",
    "# Save data. Not converting array cols to strings, storing with pickle.\n",
    "pickle.dump(data_by_split, open(save_dir+'preprocessed_data_by_split.pkl','wb'))\n",
    "print ('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StructCoder (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
